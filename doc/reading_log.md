####2016-02-05 Pennington et al., 2014; Mikolov et al., 2014; and Arora et al., 2015

Pennington et al. propose a model that utilizes the count data while capturing the linear substructures prevalent in log-bilinear prediction-based models such as word2vec. Their weighted least squares model, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations.  It thus combines the strengths of two main model families: 1) global matrix factorization methods such as LSA 2) local context window methods such as skip-gram model. The word vector learning is performed by using the ratios of co-occurrence probabilities rather than the probabilities themselves. They also show that the objective function of window-based methods such as skipgram and ivLBL is a weighted sum of cross-entropy error and bears resemblance to their weighted least squares objective function. 

Mikolov et al. describe an alternative to hierarchical softmax method called negative sampling for learning distributed vector representations that capture syntactic and semantic word relationships. Speedup in training time is performed by sub-sampling of frequent words. In addition, they introduce a variant of Noise Contrastive Estimation (NCE) for training the skip-gram model.  NCE assumes that a good model should be able to differentiate data from noise by means of logistic regression. Thus, the task is to distinguish the target word from a finite number of negative samples for each data sample (draws from the noise distribution).

Arora et al. present a generative model that provides insight into word embedding methods which employ nonlinear operations on co-occurrence statistics, such as computing Pairwise Mutual Information. The proposed generative model is a dynamic version of the loglinear topic model and helps explain why linear algebraic structure arises in low-dimensional semantic embeddings. The dynamic element of the model is a random walk over a latent discourse space (co-ordinates represent what is being talked about) and a key assumption is that the vectors have no preferred direction in space. The learned latent vector for a word is correlated with the discourse vector which is slow to change in its neighborhood. 

####2016-01-29 Bengio et al.,2003
This paper introduces a statistical language model which jointly learns a probability function of sequences of words in text and a distributed representation for words. The model is able to generalize as a sequence of words which is not present in the training set gets a high probability if it contains words that are similar to words in the sentences present in the training set. The model improves upon the state-of-the-art n-gram models by taking into contexts farther than 1 or 2 words and accounting for the similarity between words. The joint probability is decomposed as a product of conditional probabilities which are learned using a neural network. For large data sets, the paper also provides implementation details for data-parallel and parameter-parallel processing. 
