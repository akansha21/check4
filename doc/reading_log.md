This paper introduces a statistical language model which jointly learns a probability function of sequences of words in text and a distributed representation for words. The model is able to generalize as a sequence of words which is not present in the training set gets a high probability if it contains words that are similar to words in the sentences present in the training set. The model improves upon the state-of-the-art n-gram models by taking into contexts farther than 1 or 2 words and accounting for the similarity between words. The joint probability is decomposed as a product of conditional probabilities which are learned using a neural network. For large data sets, the paper also provides implementation details for data-parallel and parameter-parallel processing. 
