####2016-19-05 Levy and Goldberg 2014; Le and Mikolov 2014; and Arora et al., 2016

The common paradigm for vector representation methods for words is the distributional hypothesis of Harris, which states that words in similar contexts have similar meaning. Levy and Goldberg (2014) present that skip-gram with negative sampling (SGNS) is implicitly factorizing a word-context matrix whose cells are pointwise mutual information (PMI) of the respective word and context pairs, shifted by a constant. They also show that the Noise Constrastive Estimation (NCE) method is also factorizing a similar matrix in which each cell corresponds to the log conditional probability of a word given its context. They present an SVD algorithm to obtain dense low-dimensional vectors that achieve comparable performance to SGNS for word similarity tasks. For word-analogy tasks, SGNS performs better as it considers a weighted matrix factorization, with more weight for frequent pairs, as compared to SVD which assigns the same weight to all matrix cells. 


Le and Mikolov (2014) present the Paragraph Vector method which is an unsupervised framework that learns continuous distributed vector representations for pieces of texts of variable length. Their approach addresses the weaknesses of the bag-of-words method â€“ not accounting for word ordering and ignoring semantics of the words. The vector representation is trained by the stochastic gradient descent and backpropagation algorithms for predicting words in a paragraph. Thus, despite the fact that the vectors are initialized randomly, they capture semantics as an indirect result of the predictive tasks. The paragraph vector and word vectors are concatenated to predict the next word in a context. The paragraph vector can be seen as capturing the topic of the paragraph. 


Arora et al. (2016) present the case of word embeddings for polysemous words. They show that multiple word senses reside in a linear superposition within the word embedding and can be recovered by sparse coding. Their method is explained by an earlier model on random walk on discourses. The sparse coding ends up writing the word vectors as weighted sums of five fairly different basis vectors with which it has a positive inner product. In practice, these basis vectors correspond to the different senses of the word. Each word sense is also accompanied by one of about 2000 discourse vectors that give a concise description of its neighborhood. This makes it possible to use this approach for automated creation of WordNets in other languages.  Lastly, the use of logarithm in the embedding methods serves as a key to the success of their approach as the logarithm allows infrequent senses to have a superproportionate weight in the linear supposition.  


####2016-02-05 Pennington et al., 2014; Mikolov et al., 2014; and Arora et al., 2015

Pennington et al. propose a model that utilizes the count data while capturing the linear substructures prevalent in log-bilinear prediction-based models such as word2vec. Their weighted least squares model, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations.  It thus combines the strengths of two main model families: 1) global matrix factorization methods such as LSA 2) local context window methods such as skip-gram model. The word vector learning is performed by using the ratios of co-occurrence probabilities rather than the probabilities themselves. They also show that the objective function of window-based methods such as skipgram and ivLBL is a weighted sum of cross-entropy error and bears resemblance to their weighted least squares objective function. 

Mikolov et al. describe an alternative to hierarchical softmax method called negative sampling for learning distributed vector representations that capture syntactic and semantic word relationships. Speedup in training time is performed by sub-sampling of frequent words. In addition, they introduce a variant of Noise Contrastive Estimation (NCE) for training the skip-gram model.  NCE assumes that a good model should be able to differentiate data from noise by means of logistic regression. Thus, the task is to distinguish the target word from a finite number of negative samples for each data sample (draws from the noise distribution).

Arora et al. present a generative model that provides insight into word embedding methods which employ nonlinear operations on co-occurrence statistics, such as computing Pairwise Mutual Information. The proposed generative model is a dynamic version of the loglinear topic model and helps explain why linear algebraic structure arises in low-dimensional semantic embeddings. The dynamic element of the model is a random walk over a latent discourse space (co-ordinates represent what is being talked about) and a key assumption is that the vectors have no preferred direction in space. The learned latent vector for a word is correlated with the discourse vector which is slow to change in its neighborhood. 

####2016-01-29 Bengio et al.,2003
This paper introduces a statistical language model which jointly learns a probability function of sequences of words in text and a distributed representation for words. The model is able to generalize as a sequence of words which is not present in the training set gets a high probability if it contains words that are similar to words in the sentences present in the training set. The model improves upon the state-of-the-art n-gram models by taking into contexts farther than 1 or 2 words and accounting for the similarity between words. The joint probability is decomposed as a product of conditional probabilities which are learned using a neural network. For large data sets, the paper also provides implementation details for data-parallel and parameter-parallel processing. 
